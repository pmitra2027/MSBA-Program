---
title: "IS 6489 Final Project"
author: "Owen Horne, Michael Davis, Priyanka Mitra"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T)
library(knitr)
library(tidyr)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(arm)
library(caret)
library(Hmisc)
library(missForest)
library(psych)
library(cowplot)
library(gridExtra)

train <- read.csv("train.csv")
test <- read.csv("test.csv")

data <- rbind(train[,-81],test)

missing_values<-data.frame(Variable = names(data), Number_of_Missing_values = colSums(is.na(data)))

rownames(missing_values)<-NULL
missing_values
```

# Introduction

The aim of this project is to predict the final sales prices for homes in Ames,Iowa. The first part of the project required us to submit an interim report using only five predictors.  For that prediction, we created a linear model based on the historical data of the houses in the area. The training  dataset  consisted of  1460 observations and 80 explanatory variables. The test data consisted of 1459 observations and all the predictor variables except Sales Price, the outcome variable.

For the final project we further evaluated the training dataset and through cleaning, combining of variables, and impuatation created a better set of training data. We then began to build models with which we were able to significantly improve our predictions over the interim model.

# Exploratory Data Analysis and Cleaning

The dataset consists of both character and integer variables, where most of the character variables are actually factors.In total, there are 81 variables (character/integer), where the last variable is the Outcome variable(SalePrice).

The first part of the project consists of Data Pre-Processing. It is necessary to have a clean training data so that our final model will have greater chances of having better performance.Data Pre-Processing can be divided into 3 major parts:

- Detecting and dealing with missing values
- Normalizing distribution of predictor variable
- Handling outliers

**1. Detecting and Dealing with Missing Values**

First of all , we detected all the missing values in each column and found that there are more missing values in categorical variables than numeric variables, and the highest missing value percentage is more than 80%. For those columns with highest amount of missing values , we decided to drop the variables and by doing so our data and model performance will not be affected.

```{r echo = F}

missing_values %>%
  filter(Number_of_Missing_values>=.80*2919) %>%
   kable(caption = "\\label{tab:report1}Table Displaying Variables having more 80% NA values")

```

**Filling NA Values**

Many variables had NA values that needed to dealt with and were filled with values which made the most sense.For example, in categorical variable such as GarageType , the NAs represented an absence of garage in the property.Therefore , the NA values have been replaced with string 'None' indicating No Garage. Similarly , the NA values for other similar categorical features have been replaced with 'None'.For the numerical featureslike GarageYrBl, NA values will be replaced by 0.

**Removing Variables with a Single Dominant Feature**

On further analysis , we foud that there are many categorical variables which have one feature as the major feature i.e it dominates over more than 80% of data for that particular column. For example, consider the variable *Street* which has "Pavement" as the dominant feature and contributes to 99% of the data. 

Keeping such variables in the model will not contribute much to the model and its performance  and hence , we decided to remove such variables from model so that the number of variables in the final model is optimal and clean.The below plot shows the graphical representation of the all the variables which have a single dominant feature


```
```{r echo =F}
par(mfrow=c(3,3))
barplot(table(data$Utilities),main = "Feature Distribution of Utilities")
barplot(table(data$Street),main= "Feature Distribution of Street")
barplot(table(data$LandContour), main = "Feature Distribution of Land Contour")
barplot(table(data$Condition1) , main = "Feature Distribution of Condition1")
barplot(table(data$Condition2) , main = "Feature Distribution of Condition2")
barplot(table(data$BldgType) , main = "Feature Distribution of BuildingType")
barplot(table(data$RoofMatl) , main = "Feature Distribution of RoofMatl")
barplot(table(data$ExterCond) ,main = "Feature Distribution of ExterCond")
barplot(table(data$BsmtCond) , main = "Feature Distribution of BsmtCond")

par(mfrow=c(3,3))
barplot(table(data$BsmtFinType2) , main = "Feature Distribution of BsmtFinType2")
barplot(table(data$Heating) , main = "Feature Distribution of Heating")
barplot(table(data$CentralAir) ,main = "Feature Distribution of Central Air")
barplot(table(data$Electrical) , main = "Feature Distribution ofElectrical")
barplot(table(data$Functional) , main = "Feature Distribution of Functional")
barplot(table(data$GarageQual) , main = "Feature Distribution of GarageQual")
barplot(table(data$GarageCond) , main = "Feature Distribution of GarageCond")
barplot(table(data$PavedDrive) , main="Feature Distribution of PavedDrive")
barplot(table(data$SaleType) , main = "Feature Distribution of SaleType")
```

**Converting Numerical Variables to Factors**
There are 3 variables that are recorded as numeric but actually should be categorical.

- Month and Year Sold : For both the variables, converting them into factors makes much more sense than keeping them as integers.Converting the Year Sold into factors will group the houses sold in that particular year together.This will make modelling easier and explain the variance in SalePrice for each year.Also, changing Month Sold into factors will describe the effect of seasonality on the SalePrice.

- MSSubClass: MSSubClass identifies the type of dwelling involved in the sale.These classes are coded as numbers ,but are really categories.Fo example, MSSubClass 80 represents "Split or Multi-Level" dwelling. 

**Addition of New Variables ** 

On further exploration of the housing data ,we decided to add 7 more variables in the dataset. This variables depict an important feature , based on the combination of variables already present in the dataset. The variables added in the dataset can be explained as follows:

1.TotalSqFeet : The new variable created is one of the most important features of the house which gives the total squarefeet of the house and makes it much easier to be compared with other house areas.

2.TotBathrooms : Instead of comparing each variable related to feature Bathroom of the house, a variable Totbathrroms has been added which denote the Total Number of Bathrooms in the house.This variable is made of the Bathrooms of the house and the ones present in the Basement.

3.TotalPorchSF : Similar to TotalSqFeet , this variable provides information about the Total Porch Area of a house.

4.PropAge : Calculated from YearSold and YearBuilt, the PropAge(Property Age) will help in determining whether the value of a property should increase or decrease based on its value.

5.Remod : This is a boolean variable which will give output 1 if the property was remodelled and 0 if the property was not remodelled.

6.LastRemod : This variable will provide information that if a particular property was remodelled , then which year it was re-modelled. This feature will also affect the SalePrice of the house.

7.IsNew : For our data, we have considered that if the difference between YearSold and YearBuilt is less than 15 years , then the property is a new property (denoted as 1)or else it will be old (denoted as 0).

**2. Normalizing distribution of target variable**

First, the regression requires all variables in the model to have normal distributions.Therefore, we created histogram for our predictor Sales Price to analyze the distribution.From the plot below, we could see the distribution of sale price clearly skewed to the right and one way to deal with skewed data is to log-transform the variable.


```{r}
ggplot(train, aes(x = SalePrice,fill = ..count..)) +
  geom_histogram() +
  ggtitle("Figure 1 Histogram of SalePrice") +
  ylab("Count of houses") +
  xlab("Housing Price") + 
  theme(plot.title = element_text(hjust = 0.5))
```

The histogram is plotted again after the sales price is log-transformed and its distribution is almost the shape of the normal distribution.Before we fit the regression model , we pre-processed with log-transform of Sale Price and we are going to use the logged Sale Price for our final model.

```{r}
train$lSalePrice <- log(train$SalePrice)
# Draw a higtogram to figure out the distribution of log SalePrice

ggplot(train, aes(x = lSalePrice, fill = ..count..)) +
  geom_histogram() +
  ggtitle("Figure 2 Histogram of log SalePrice") +
  ylab("Count of houses") +
  xlab("Housing Price") + 
  theme(plot.title = element_text(hjust = 0.5))
```



# Imputation

Missing values in data sets are problematic in data analysis, model training and prediction. Hence, we will impute missing values before going further in model training. For our project, we have imputed missing values using the MissForest package which will do single imputations using random forest.We haven't used other imputation methods such as knnImpute or medianImpute as they work for only numeric variables and we will have to convert the factors to Integers.


# Model Development and Variable Selection

# Prediction Results and Fit Metrics


